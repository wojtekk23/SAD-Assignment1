---
title: "Zadanie zaliczeniowe 1"
author: "Wojciech Kłopotek"
date: "29 04 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Treść zadania

**Celem** zadania jest staystyczna analiza danych znajdujących się w pliku `people.tab`.

**Dane:** Są to dane symulowane; opisują wiek (zmienna `age`), wagę (`weight`), wzrost (`height`), płeć (`gender`), stan cywilny (`married`), liczbę dzieci (`number_of_kids`), posiadane zwierzę domowe (`pet`) oraz miesięczne wydatki (`expenses`) pewnych osób. We wszystkich zadaniach poniżej zmienna `expenses` jest **zmienną objaśnianą** (zależną), a pozostałe zmienne są **zmiennymi objaśniającymi** (niezależnymi).

**Wynikiem** ma być raport w formacie .Rmd oraz skompilowany do html. Raport w obydwu formatach należy przesłać na adres email do prwoadzącego laboratorium do sprawdzenia.

# Zadanie 1

*Wczytaj dane, objerzyj je i podsumuj w dwóch-trzech zdaniach. Pytania pomocnicze: ile jest obserwacji, ile zmiennych ilościowych, a ile jakościowych? Czy są zależności w zmiennych objaśniających (policz i zaprezentuj na wykresach korelacje pomiędzy zmiennymi ilościowymi), a także zbadaj zależność zmiennych jakościowych. Skomentuj wyniki. Czy występują jakieś braki danych?*

```{r}
df <- read.csv("people.tab", sep="\t")
```

Zbiór danych zawiera `r nrow(df)` obserwacji, 3 zmienne jakościowe i 5 zmiennych ilościowych.

```{r}
library(ggplot2)
library(GGally)

#ggpairs(df[,-which(colnames(df)=='expenses')], aes(col=gender))
ggpairs(df, columns = c(1,2,3,6,8))
ggpairs(df, aes(col=gender), columns = c(1,2,3,6,8))
```


Widzimy statystycznie znaczące korelacje między zmiennymi `expenses` i `age` (wydatkami a wiekiem) oraz `height` i `weight` (wzrostem i wagą) dla zagregowanych danych oraz dla wszystkich płci osobno. Powyższe wykresy wskazują na to, że wiek jest znacząco skorelowany z wydatkami.

```{r}
ggpairs(df[,c(4,5,7)])
```

W danych występują pojedyncze braki:
```{r}
colSums(is.na(df))
```

Konkretnie zbiór danych zawiera `r length(which(rowSums(is.na(df)) > 0))` wybrakowanych wierszy.

# Zadanie 2

***Podsumuj dane przynajmniej trzema różnymi wykresami**. Należy przygotować:*

* *wykres typu scatter-plot (taki jak na wykładzie 6, slajd 3) dla wszystkich zmiennych objaśniających ilościowych i zmiennej objaśnianej*
* *Wykresy typu pudełkowy (boxplot) dla jednej wybranej zmiennej ilościowej.*
* *Wykres typu słupkowy (barplot) dla jednej wybranej zmiennej jakościowej.*

*Mile widziane dodatkowe wykresy wg własnej inwencji (np. histogram, punktowy, liniowy, mapa ciepła, ...)*

```{r}
pairs(~ age + weight + height + number_of_kids + expenses, data = df, col = 'blue', pch='.')
```

```{r}
library(ggplot2)

ggplot(df) + geom_boxplot(aes(y = age))
```

```{r}
# Usuwamy niepoprawny wiersz
ggplot(df[-nrow(df),]) + geom_bar(aes(x = pet))
```

```{r}
df = df[-which(rowSums(is.na(df)) > 0),]
```

# Zadanie 3

***Policz p-wartości dla hipotez o wartości średniej $m = 170$ i medianie $me = 165$ (cm)** dla zmiennej $wzrost$. Wybierz statystykę testową dla alternatywy lewostronnej, podaj założenia, z jakich korzystałeś i skomentuj, czy wydają Ci się uprawnione*

Założę, że średnia zmiennej $wzrost$ ma rozkład normalny, ze względu na to, że obserwacje pochodzą z tego samego rozkładu i są niezależne, więc ze względu na dużą ich liczbę (`r nrow(df)` > 30) możemy użyć Centralnego Twierdzenia Granicznego. Ze względu na to, że nie znamy wariancji populacji, ale liczność próby jest liczna ($n = $`r nrow(df)`) użyjemy wariancji próby i policzymy statystykę testową dla standardowego rozkładu normalnego:

$$
Z = \frac{\bar{X} - m}{S_n}\sqrt{n} \sim U
$$

```{r}
m <- 170
n <- nrow(df)
S_n <- sqrt((n-1)/n) * sd(df$height)
Z <- (mean(df$height) - m) / S_n * sqrt(nrow(df))
pnorm(Z)
```

Dla mediany możemy przeprowadzić test Wilcoxona dla jednej próby

```{r}
wilcox.test(df$height, mu=165)
```

# Zadanie 4
***Policz dwustronne przedziały ufności** na poziomie $0.99$ dla zmiennej $wiek$ dla następujących parametrów rozkładu:*
1. *średnia i odchylenie standardowe*
2. *kwantyle $\frac{1}{4}$, $\frac{2}{4}$ i $\frac{3}{4}$*
*Podaj założenia, z jakich korzystałeś i skomentuj, czy wydają Ci się uprawnione.*

Z wykresu Q-Q plot dla zmiennej $wiek$ wynika, że ma ona rozkład normalny. (Wykres jest liniowy)

```{r}
df.woman <- df$gender == "woman"
df.man <- df$gender == "man"
qqnorm(df$age); qqline(df[df.man,]$age, col=2)
```

Możemy przetestować to bardziej rygorystycznie korzystając z testu Kołmogorowa lub testu Shapiro-Wilka (TODO: MAŁY EFEKT TEŻ JEST STATYStYCZNIE ZNACZĄCY, *Like most statistical significance tests, if the sample size is sufficiently large this test may detect even trivial departures from the null hypothesis (i.e., although there may be some statistically significant effect, it may be too small to be of any practical significance); thus, additional investigation of the effect size is typically advisable, e.g., a Q–Q plot in this case*)

```{r}
ks.test((df[df.man,]$age - mean(df[df.man,]$age)) / sd(df[df.man,]$age), "pnorm")
shapiro.test(df$age)
```

```{r}
ggplot(df) + geom_histogram(aes(x=(age - mean(age)) / sd(age))) + geom_histogram(aes(x=rnorm(n=500)), color='red') + theme_minimal()
hist((df$age - mean(df$age)) / sd(df$age))
hist(rt(n=500, df=85))
```


## Przedziały ufności dla średniej

Jeśli $P\left(\frac{\bar{X}-\mu}{S_n}\sqrt{n} \in (\pm z_{0.995})\right) = 0.99$, to: $P\left(\mu \in (\bar{X} \mp z_{0.995})\right) = 0.99$. Korzystamy z Centralnego Twierdzenia Granicznego.

```{r}
n <- nrow(df)
S_n <- sqrt((n-1)/n) * sd(df$age)

show(c(mean(df$age) - S_n / sqrt(n) * qnorm(0.995), mean(df$age) + S_n / sqrt(n) * qnorm(0.995)))
```

## Przedziały ufności dla wariancji

Zakładając, że obserwacje wieku są niezależne, możemy zauważyć, że $P \left( \frac{nS_n^2}{\sigma^2} \in (\chi^2(0.005), \chi^2(0.995))\right) = 0.99$, więc: $P \left( \sigma^2 \in \left( \frac{nS_n^2}{\chi^2(0.995)}, \frac{nS_n^2}{\chi^2(0.005)} \right) \right) = 0.99$

```{r}
n <- nrow(df)
S2 <- (n-1) / n * var(df$age)

show(c( n*S2 / qchisq(0.995, n-1), n*S2 / qchisq(0.005, n-1) ))
```

## Kwantyle

Aby oszacować przedziały ufności poszczególnych kwantyli, wykorzystam metodę Bootstrap. Wezmę $B=1000$ repróbek.

```{r}
quartile.first <- replicate(1000, quantile(sample(df$age, 500, rep=T), 0.25))
quartile.second <- replicate(1000, quantile(sample(df$age, 500, rep=T), 0.5))
quartile.third <- replicate(1000, quantile(sample(df$age, 500, rep=T), 0.75))
quartiles <- matrix(c(quartile.first, quartile.second, quartile.third), ncol=3)
show(apply(quartiles, 2, mean))
```

Powyżej widzimy średnie kwartyle. Teraz policzmy przedziały ufności:

```{r}
apply(quartiles, 2, function(x) quantile(x, c(0.005, 0.995)))
```

# Zadanie 5

***Przetestuj na poziomie istotności $0.01$ trzy hipotezy istotności:***
1. *różnicy między średnią wartością wybranej zmiennej dla kobiet i dla mężczyzn*
2. *zależności między dwiema zmiennymi ilościowymi*
3. *zależności między dwiema zmiennymi jakościowymi*


## Różnica między średnią wartością wzrostu dla kobiet i dla mężczyzn

```{r}
qqnorm(df[df.woman,]$height) 
qqnorm(df[df.man,]$height)
```
Wykresy wyglądają na leżące na prostej, więc uznamy, że obie zmienne pochodzą z rozkładu normalnego.

Skorzystamy z t-testu dla dwóch średnich (testu Welcha). Przyjmujemy przy tym Nasza hipotezy:
$$
H_0: \mu_1 = \mu_2 \\
H_a: \mu_1 \not= \mu_2
$$
Użyjemy statystyki testowej o rokzładzie
$$
T = \frac{\bar{X_1} - \bar{X_2}}{\sqrt{\frac{s_1^2}{N_1} + \frac{s_2^2}{N_2}}}
$$

```{r}
t.test(df[df.woman,]$height, df[df.man,]$height, alternative="two.sided", var.equal=F, conf.level=0.99)
```
Z powyższego wynika, że dla przyjętego poziomu istotności nie możemy odrzucić hipotezy zerowej, więc przyjmujemy, że kobiety i mężczyźni mają średnio tyle samo wzrostu.

## Zależność między wzrostem a wagą

Za pomocą testu niezależności $\rho$-Spearmana, czy wzrost i waga są niezależne. Hipotezy:
$$
H_0: height \text{ i } weight \text{ są niezależne} \\
H_1: \text{istnieje zależność między } height \text{ i } weight\\
$$
```{r}
cor.test(df$height, df$weight, method="spearman", conf.level = 0.99)
```

p-wartość w powyższej funkcji została policzona korzystając z wartości rozkładu t. Tak czy siak, wartość p dla naszego modelu jest znacznie mniejsza niż wymagany przez nas poziom istotności, więc odrzucamy hipotezę zerową i mówimy, że istnieje zależność między wzrostem i wagą.

## Zależność między płcią a statusem małżeńskim

Tym razem wykorzystamy test $\chi^2$-Pearsona. Przyjmując oznaczenia brzegowych rozkładów: $P(X=x_i)=p_{i.}, P(Y=y_j)=p_{.j}$ oraz łącznego rozkładu $P(X=x_i,Y=y_j)=p_{ij}$ (gdzie $X$ uznajmy, że jest płcią, $Y$ statusem małżeńskim), badamy następujące hipotezy:
$$
H_0: p_{ij} = p_{i.}p_{.j} \\
H_1: p_{ij} \not= p_{i.}p_{.j} \text{ (zmienne zależne)}
$$
```{r}
test.table <- table(df$gender, df$married)
show(test.table)
```
```{r}
chisq.test(test.table)
```
Widzimy, że uzyskane p-value jest wyższe niż oczekiwany od nas poziom istotności, więc nie odrzucamy hipotezy zerowej i uznajemy, że płeć i status małżeński są niezależne.

# Zadanie 6

*Oszacuj model regresji liniowej, przyjmując za zmienną zależną ($y$) wydatki domowe (`expenses`) a jako zmienne niezależne ($x$) przyjmując pozostałe zmienne. Rozważ, czy konieczne są transformacje zmiennych lub zmiennej objaśniającej. Podaj RSS, R<sup>2</sup>, p-wartości i oszacowania współczynników w pełnym modelu (w modelu zawierającym wszystkie zmienne). Następnie wybierz jedną zmienną objaśniającą, którą można by z pełnego modelu odrzucić (która najgorzej tłumaczy `expenses`). Aby dokonać wyboru takiej zmiennej, dla każdej ze zmiennych objaśniających sprawdź:*

1. *Jaką ma p-wartość w pełnym modelu?*
2. *O ile zmniejsza się R<sup>2</sup>, gdy usuniemy ją z pełnego modelu?*
3. *O ile zwiększa się RSS, gdy ją usuniemy z pełnego modelu?*

*Opisz wnioski*
*Oszacuj model ze zbiorem zmiennych objaśniających pomniejszonym o wybraną zmienną. Sprawdź, czy w otrzymanym przez Ciebie modelu spełnione są założenia modelu liniowego i przedstaw na wykresach diagnostycznych: wykresie zależności reszt od zmiennej objaśnianej, na wykresie reszt studentyzowanych i na wykresie dźwigni i przedyskutuj, czy są spełnione*

```{r}
model <- lm(expenses ~ ., data = df)
summary(model)
```
Widzimy, że w pełnym modelu $RSS = 213.9$, $R^2 = 0.861$, a wartości p są widoczne powyżej.

Ze względu na wysokie p-values dobrymi kandydatami na zmienną do odrzucenia wydają się być: `weight`, `gender`, `married` i `number_of_kids`.

Najlepszym kandydatem zdaje się być zmienna `married`. Nie dość, że przyjmuje największą p-wartość, to $R^2$ praktycznie się nie zmniejsza (na pewno zmniejsza się najmniej ze wszystkich zmniejszonych modeli) i $RSE$ *zmiejsza* się ($RSS$ zwiększyło się mniej niż liczba stopni swobody).
```{r}
model.no.weight <- lm(expenses ~ . - weight, data = df)
model.no.gender <- lm(expenses ~ . - gender, data = df)
model.no.married <- lm(expenses ~ . - married, data = df)
model.no.number_of_kids <- lm(expenses ~ . - number_of_kids, data = df)
summary(model.no.weight)
summary(model.no.gender)
summary(model.no.married)
summary(model.no.number_of_kids)
```

Zatem przyjmę, że najlepiej odrzucić zmienną `married`. Sprawdzę, czy w modelu `model.no.married` spełnione są założenia regresji liniowej:

* **L**inear trend
* **I**ndependent residuals
* **N**ormally distributed residuals
* **E**qual variance of residuals

## Trend liniowy i niezależność residuów

```{r}
plot(model.no.married, which=1)
```

Z powyższego wykresu wynika, że zależność między wydatkami a zmiennymi objaśniającymi nie jest do końca liniowa.

Prawdopodobnie z tego powodu widzimy, że błędy nie są od siebie niezależne (różnica między sąsiednimi błędami jest mniejsza niż między błędami wybranymi losowo - autokorelacja).

## Rozkład residuów

```{r}
plot(model.no.married, which=2)
```

Z powyższego wykresu wynika, że residua mają w przybliżeniu rozkład normalny.

## Homoskedatyczność wariancji

```{r}
plot(model.no.married, which=3)
```

Widzimy, że wariancja residuów raczej nie zależy od zmiennej objaśnianej - utrzymuje się na mniej więcej tym samym poziomie (Odchylenia czerwonej lini trendu od prostej może wynikać z małej liczby obserwacji po obu stronach osi X na wykresie).

# Obserwacje odstające

```{r}
plot(model.no.married, which=5)
```
Widzimy 3 obserwacje odstające (reszty studentyzowane odchylają się od 0 o więcej niż 3): 305, 409, 472.

Na wykładzie za dźwigniowe zostały uznane obserwacje, dla których dźwignia jest większa od $\frac{2(p+1)}{n}$. Sprawdźmy, czy istnieją obserwacje dźwigniowe wg tej definicji.

```{r}
p <- 10
n <- 500
which(hatvalues(model.no.married) > 2*(p+1)/n)
```
Widzimy, że jedyną obserwacją zarówno odstającą jak i dźwigniową jest 409. Możemy zatem zastanowić się nad usunięciem ze zbioru danych obserwacji 305 i 472 (obserwacji odstających, niedźwigniowych)

